@article{Kremer2014,
   abstract = {In machine learning, active learning refers to algorithms that autonomously select the data points from which they will learn. There are many data mining applications in which large amounts of unlabeled data are readily available, but labels (e.g., human annotations or results coming from complex experiments) are costly to obtain. In such scenarios, an active learning algorithm aims at identifying data points that, if labeled and used for training, would most improve the learned model. Labels are then obtained only for the most promising data points. This speeds up learning and reduces labeling costs. Support vector machine (SVM) classifiers are particularly well-suited for active learning due to their convenient mathematical properties. They perform linear classification, typically in a kernel-induced feature space, which makes expressing the distance of a data point from the decision boundary straightforward. Furthermore, heuristics can efficiently help estimate how strongly learning from a data point influences the current model. This information can be used to actively select training samples. After a brief introduction to the active learning problem, we discuss different query strategies for selecting informative data points and review how these strategies give rise to different variants of active learning with SVMs. For further resources related to this article, please visit the WIREs website. Conflict of interest: The authors have declared no conflicts of interest for this article. © 2014 John Wiley & Sons, Ltd.},
   author = {Jan Kremer and Kim Steenstrup Pedersen and Christian Igel},
   doi = {10.1002/widm.1132},
   issn = {19424787},
   issue = {4},
   journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
   month = {7},
   pages = {313-326},
   publisher = {Wiley-Blackwell},
   title = {Active learning with support vector machines},
   volume = {4},
   url = {http://doi.wiley.com/10.1002/widm.1132},
   year = {2014},
}
@report{Nguyen2015,
   abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study [30] revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neu-ral networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call "fooling images" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.},
   author = {Anh Nguyen and Jason Yosinski and Jeff Clune},
   pages = {427-436},
   title = {Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images},
   url = {http://EvolvingAI.org/fooling.},
   year = {2015},
}
@article{Moon2020,
   abstract = {Despite the power of deep neural networks for a wide range of tasks, an overconfident prediction issue has limited their practical use in many safety-critical applications. Many recent works have been proposed to mitigate this issue, but most of them require either additional computational costs in training and/or inference phases or customized architectures to output confidence estimates separately. In this paper, we propose a method of training deep neural networks with a novel loss function, named Correctness Ranking Loss, which regularizes class probabilities explicitly to be better confidence estimates in terms of ordinal ranking according to confidence. The proposed method is easy to implement and can be applied to the existing architectures without any modification. Also, it has almost the same computational costs for training as conventional deep classifiers and outputs reliable predictions by a single inference. Extensive experimental results on classification benchmark datasets indicate that the proposed method helps networks to produce well-ranked confidence estimates. We also demonstrate that it is effective for the tasks closely related to confidence estimation, out-of-distribution detection and active learning.},
   author = {Jooyoung Moon and Jihyo Kim and Younghak Shin and Sangheum Hwang},
   journal = {arXiv},
   month = {7},
   publisher = {arXiv},
   title = {Confidence-Aware Learning for Deep Neural Networks},
   url = {http://arxiv.org/abs/2007.01458},
   year = {2020},
}
@techreport{Settles2009,
Author = {Burr Settles},
Institution = {University of Wisconsin--Madison},
Number = {1648},
Title = {Active Learning Literature Survey},
Type = {Computer Sciences Technical Report},
Year = {2009},
}
@article{Balakrishnan2020,
   abstract = {Empirical evidences linking users’ psychological features such as personality traits and cybercrimes such as cyberbullying are many. This study deals with automatic cyberbullying detection mechanism tapping into Twitter users’ psychological features including personalities, sentiment and emotion. User personalities were determined using Big Five and Dark Triad models, whereas machine learning classifiers namely, Naïve Bayes, Random Forest and J48 were used to classify the tweets into one of four categories: bully, aggressor, spammer and normal. The Twitter dataset contained 5453 tweets gathered using the hashtag #Gamergate, and manually annotated by human experts. Selected Twitter-based features namely text, user and network-based features were used as the baseline algorithm. Results show that cyberbullying detection improved when personalities and sentiments were used, however, a similar effect was not observed for emotion. A further analysis on the personalities revealed extraversion, agreeableness, neuroticism and psychopathy to have greater impacts in detecting online bullying compared to other traits. Key features were identified using the dimension reduction technique, and integrated into a single model, which produced the best detection accuracy. The paper describes suggestions and recommendations as to how the findings can be applied to mitigate cyberbullying.},
   author = {Vimala Balakrishnan and Shahzaib Khan and Hamid R. Arabnia},
   doi = {10.1016/j.cose.2019.101710},
   issn = {01674048},
   journal = {Computers and Security},
   keywords = {Cyberbullying,Emotion,Machine learning,Personality,Sentiment,Twitter},
   month = {3},
   pages = {101710},
   publisher = {Elsevier Ltd},
   title = {Improving cyberbullying detection using Twitter users’ psychological features and machine learning},
   volume = {90},
   year = {2020},
}
@inproceedings{Dadvar2013,
   abstract = {The negative consequences of cyberbullying are becoming more alarming every day and technical solutions that allow for taking appropriate action by means of automated detection are still very limited. Up until now, studies on cyberbullying detection have focused on individual comments only, disregarding context such as users' characteristics and profile information. In this paper we show that taking user context into account improves the detection of cyberbullying. © 2013 Springer-Verlag.},
   author = {Maral Dadvar and Dolf Trieschnigg and Roeland Ordelman and Franciska De Jong},
   doi = {10.1007/978-3-642-36973-5_62},
   isbn = {9783642369728},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {693-696},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Improving cyberbullying detection with user context},
   volume = {7814 LNCS},
   url = {http://www.noswearing.com/dictionary},
   year = {2013},
}
@article{Muneer2020,
   abstract = {<p>The advent of social media, particularly Twitter, raises many issues due to a misunderstanding regarding the concept of freedom of speech. One of these issues is cyberbullying, which is a critical global issue that affects both individual victims and societies. Many attempts have been introduced in the literature to intervene in, prevent, or mitigate cyberbullying; however, because these attempts rely on the victims’ interactions, they are not practical. Therefore, detection of cyberbullying without the involvement of the victims is necessary. In this study, we attempted to explore this issue by compiling a global dataset of 37,373 unique tweets from Twitter. Moreover, seven machine learning classifiers were used, namely, Logistic Regression (LR), Light Gradient Boosting Machine (LGBM), Stochastic Gradient Descent (SGD), Random Forest (RF), AdaBoost (ADB), Naive Bayes (NB), and Support Vector Machine (SVM). Each of these algorithms was evaluated using accuracy, precision, recall, and F1 score as the performance metrics to determine the classifiers’ recognition rates applied to the global dataset. The experimental results show the superiority of LR, which achieved a median accuracy of around 90.57%. Among the classifiers, logistic regression achieved the best F1 score (0.928), SGD achieved the best precision (0.968), and SVM achieved the best recall (1.00).</p>},
   author = {Amgad Muneer and Suliman Mohamed Fati},
   doi = {10.3390/fi12110187},
   issn = {1999-5903},
   issue = {11},
   journal = {Future Internet},
   keywords = {AdaBoost,Cyberbullying detection,Light GBM,Logistic regression,Naive bayes,Random forest,SGD,SVM,Tweets classification,Twitter},
   month = {10},
   pages = {187},
   publisher = {MDPI AG},
   title = {A Comparative Analysis of Machine Learning Techniques for Cyberbullying Detection on Twitter},
   volume = {12},
   url = {https://www.mdpi.com/1999-5903/12/11/187},
   year = {2020},
}
@article{Rosa2019,
   abstract = {Automatic cyberbullying detection is a task of growing interest, particularly in the Natural Language Processing and Machine Learning communities. Not only is it challenging, but it is also a relevant need given how social networks have become a vital part of individuals' lives and how dire the consequences of cyberbullying can be, especially among adolescents. In this work, we conduct an in-depth analysis of 22 studies on automatic cyberbullying detection, complemented by an experiment to validate current practices through the analysis of two datasets. Results indicated that cyberbullying is often misrepresented in the literature, leading to inaccurate systems that would have little real-world application. Criteria concerning cyberbullying definitions and other methodological concerns seem to be often dismissed. Additionally, there is no uniformity regarding the methodology to evaluate said systems and the natural imbalance of datasets remains an issue. This paper aims to direct future research on the subject towards a viewpoint that is more coherent with the definition and representation of the phenomenon, so that future systems can have a practical and impactful application. Recommendations on future works are also made.},
   author = {H. Rosa and N. Pereira and R. Ribeiro and P. C. Ferreira and J. P. Carvalho and S. Oliveira and L. Coheur and P. Paulino and A. M. Veiga Simão and I. Trancoso},
   doi = {10.1016/j.chb.2018.12.021},
   issn = {07475632},
   journal = {Computers in Human Behavior},
   keywords = {Abusive language,Automatic cyberbullying detection,Cyberbullying,Machine learning,Natural language processing,Social networks},
   month = {4},
   pages = {333-345},
   publisher = {Elsevier Ltd},
   title = {Automatic cyberbullying detection: A systematic review},
   volume = {93},
   year = {2019},
}
@inproceedings{Zhao2016,
   abstract = {With the increasing use of social media, cyberbullying behaviour has received more and more attention. Cyberbullying may cause many serious and negative impacts on a person's life and even lead to teen suicide. To reduce and stop cyberbullying, one effective solution is to automatically detect bullying content based on appropriate machine learning and natural language processing techniques. However, many existing approaches in the literature are just normal text classification models without considering bullying characteristics. In this paper, we propose a representation learning framework specific to cyberbullying detection. Based on word embeddings, we expand a list of pre-defined insulting words and assign different weights to obtain bullying features, which are then concatenated with Bag-of-Words and latent semantic features to form the final representation before feeding them into a linear SVM classifier. Experimental study on a twitter dataset is conducted, and our method is compared with several baseline text representation learning models and cyberbullying detection methods. The superior performance achieved by our method has been observed in this study.},
   author = {Rui Zhao and Anna Zhou and Kezhi Mao},
   city = {New York, New York, USA},
   doi = {10.1145/2833312.2849567},
   isbn = {9781450340328},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Bag-of-Words,Cyberbullying detection,Representation learning,Text mining,Word embeddings},
   month = {1},
   pages = {1-6},
   publisher = {Association for Computing Machinery},
   title = {Automatic detection of cyberbullying on social networks based on bullying features},
   volume = {04-07-January-2016},
   url = {http://dl.acm.org/citation.cfm?doid=2833312.2849567},
   year = {2016},
}
@article{garadi-highestf/top10features,
   abstract = {The popularity of online social networks has created massive social communication among their users and this leads to a huge amount of user-generated communication data. In recent years, Cyberbullying has grown into a major problem with the growth of online communication and social media. Cyberbullying has been recognized recently as a serious national health issue among online social network users and developing an efficient detection model holds tremendous practical significance. In this paper, we have proposed set of unique features derived from Twitter; network, activity, user, and tweet content, based on these feature, we developed a supervised machine learning solution for detecting cyberbullying in the Twitter. An evaluation demonstrates that our developed detection model based on our proposed features, achieved results with an area under the receiver-operating characteristic curve of 0.943 and an f-measure of 0.936. These results indicate that the proposed model based on these features provides a feasible solution to detecting Cyberbullying in online communication environments. Finally, we compare result obtained using our proposed features with the result obtained from two baseline features. The comparison outcomes show the significance of the proposed features.},
   author = {Mohammed Ali Al-Garadi and Kasturi Dewi Varathan and Sri Devi Ravana},
   doi = {10.1016/j.chb.2016.05.051},
   issn = {07475632},
   journal = {Computers in Human Behavior},
   keywords = {Cyberbullying,Cybercrime,Machine learning,Online communication,Online social networks,Twitter},
   month = {10},
   pages = {433-443},
   publisher = {Elsevier Ltd},
   title = {Cybercrime detection in online communications: The experimental case of cyberbullying detection in the Twitter network},
   volume = {63},
   year = {2016},
},
@article{Zhao2017,
   abstract = {As a side effect of increasingly popular social media, cyberbullying has emerged as a serious problem afflicting children, adolescents and young adults. Machine learning techniques make automatic detection of bullying messages in social media possible, and this could help to construct a healthy and safe social media environment. In this meaningful research area, one critical issue is robust and discriminative numerical representation learning of text messages. In this paper, we propose a new representation learning method to tackle this problem. Our method named semantic-enhanced marginalized denoising auto-encoder (smSDA) is developed via semantic extension of the popular deep learning model stacked denoising autoencoder (SDA). The semantic extension consists of semantic dropout noise and sparsity constraints, where the semantic dropout noise is designed based on domain knowledge and the word embedding technique. Our proposed method is able to exploit the hidden feature structure of bullying information and learn a robust and discriminative representation of text. Comprehensive experiments on two public cyberbullying corpora ( Twitter and MySpace) are conducted, and the results show that our proposed approaches outperform other baseline text representation learning methods.},
   author = {Rui Zhao and Kezhi Mao},
   doi = {10.1109/TAFFC.2016.2531682},
   issn = {19493045},
   issue = {3},
   journal = {IEEE Transactions on Affective Computing},
   keywords = {Cyberbullying detection,representation learning,stacked denoising autoencoders,text mining,word embedding},
   month = {7},
   pages = {328-339},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Cyberbullying Detection Based on Semantic-Enhanced Marginalized Denoising Auto-Encoder},
   volume = {8},
   year = {2017},
},
@inproceedings{Singh2016,
   abstract = {Cyberbullying is an important socio-technical challenge in Online Social Networks (OSN). With the growth trends of heterogeneous data in OSN, better network characterization, and textual feature sophistication, recent efforts have realized the value of looking at heterogeneous modes of information including textual features, social features, and image-based features for better cyberbullying detection. These approaches, however, still use these features either individually or combine them 'naively' without considering the different confidence levels associated with each feature or the interdependencies between features. We propose a novel probabilistic information fusion framework that utilizes confidence score and interdependencies associated with different social and textual features and uses those to build better predictors for cyberbullying. The performance of the proposed approach was compared to a recent approach in literature which used a similar dataset and features and the proposed approach resulted in significant improvements in terms of cyberbullying detection.},
   author = {Vivek K. Singh and Qianjia Huang and Pradeep K. Atrey},
   doi = {10.1109/ASONAM.2016.7752342},
   isbn = {9781509028467},
   journal = {Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2016},
   month = {11},
   pages = {884-887},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Cyberbullying detection using probabilistic socio-textual information fusion},
   year = {2016},
},
@inproceedings{schumann-rehbein-2019-active,
    title = "Active Learning via Membership Query Synthesis for Semi-Supervised Sentence Classification",
    author = "Schumann, Raphael  and
      Rehbein, Ines",
    booktitle = "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/K19-1044",
    doi = "10.18653/v1/K19-1044",
    pages = "472--481",
    abstract = "Active learning (AL) is a technique for reducing manual annotation effort during the annotation of training data for machine learning classifiers. For NLP tasks, pool-based and stream-based sampling techniques have been used to select new instances for AL while gen erating new, artificial instances via Membership Query Synthesis was, up to know, considered to be infeasible for NLP problems. We present the first successfull attempt to use Membership Query Synthesis for generating AL queries, using Variational Autoencoders for query generation. We evaluate our approach in a text classification task and demonstrate that query synthesis shows competitive performance to pool-based AL strategies while substantially reducing annotation time",
},
@inproceedings{hateoffensive,
  title = {Automated Hate Speech Detection and the Problem of Offensive Language},
  author = {Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar}, 
  booktitle = {Proceedings of the 11th International AAAI Conference on Web and Social Media},
  series = {ICWSM '17},
  year = {2017},
  location = {Montreal, Canada},
  pages = {512-515}
  },
@article{modAL2018,
    title={mod{AL}: {A} modular active learning framework for {P}ython},
    author={Tivadar Danka and Peter Horvath},
    url={https://github.com/modAL-python/modAL},
    note={available on arXiv at https://arxiv.org/abs/1805.00979}
},
@inproceedings{pennington2014glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543},
  url = {http://www.aclweb.org/anthology/D14-1162},
},
@INPROCEEDINGS{Platt99probabilisticoutputs,
    author = {John C. Platt},
    title = {Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods},
    booktitle = {ADVANCES IN LARGE MARGIN CLASSIFIERS},
    year = {1999},
    pages = {61--74},
    publisher = {MIT Press}
},
@Article{Boot2019,
author={Boot, Arnout B.
and Tjong Kim Sang, Erik
and Dijkstra, Katinka
and Zwaan, Rolf A.},
title={How character limit affects language usage in tweets},
journal={Palgrave Communications},
year={2019},
month={Jul},
day={09},
volume={5},
number={1},
pages={76},
abstract={In November 2017 Twitter doubled the available character space from 140 to 280 characters. This provided an opportunity for researchers to investigate the linguistic effects of length constraints in online communication. We asked whether the character limit change (CLC) affected language usage in Dutch tweets and hypothesized that there would be a reduction in the need for character-conserving writing styles. Pre-CLC tweets were compared with post-CLC tweets. Three separate analyses were performed: (I) general analysis: the number of characters, words, and sentences per tweet, as well as the average word and sentence length. (II) Token analysis: the relative frequency of tokens and bigrams; (III) part-of-speech analysis: the grammatical structure of the sentences in tweets (i.e., adjectives, adverbs, articles, conjunctives, interjections, nouns, prepositions, pronouns, and verbs); pre-CLC tweets showed relatively more textisms, which are used to abbreviate and conserve character space. Consequently, they represent more informal language usage (e.g., internet slang); in turn, post-CLC tweets contained relatively more articles, conjunctions, and prepositions. The results show that online language producers adapt their texts to overcome limit constraints.},
issn={2055-1045},
doi={10.1057/s41599-019-0280-3},
url={https://doi.org/10.1057/s41599-019-0280-3}
},
@article{SINGH2016549,
title = {Role of Text Pre-processing in Twitter Sentiment Analysis},
journal = {Procedia Computer Science},
volume = {89},
pages = {549-554},
year = {2016},
note = {Twelfth International Conference on Communication Networks, ICCN 2016, August 19– 21, 2016, Bangalore, India Twelfth International Conference on Data Mining and Warehousing, ICDMW 2016, August 19-21, 2016, Bangalore, India Twelfth International Conference on Image and Signal Processing, ICISP 2016, August 19-21, 2016, Bangalore, India},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.06.095},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916311607},
author = {Tajinder Singh and Madhu Kumari},
keywords = {Classification, CRF, n-Gram, Sentiment, Text Pre-Processing.},
abstract = {Ubiquitous nature of online social media and ever expending usage of short text messages becomes a potential source of crowd wisdom extraction especially in terms of sentiments therefore sentiment classification and analysis is a significant task of current research purview. Major challenge in this area is to tame the data in terms of noise, relevance, emoticons, folksonomies and slangs. This works is an effort to see the effect of pre-processing on twitter data for the fortification of sentiment classification especially in terms of slang word. The proposed method of pre-processing relies on the bindings of slang words on other coexisting words to check the significance and sentiment translation of the slang word. We have used n-gram to find the bindings and conditional random fields to check the significance of slang word. Experiments were carried out to observe the effect of proposed method on sentiment classification which clearly indicates the improvements in accuracy of classification.}
},
@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
},
@ARTICLE{Chawla02smote:synthetic,
    author = {Nitesh V. Chawla and Kevin W. Bowyer and Lawrence O. Hall and W. Philip Kegelmeyer},
    title = {SMOTE: Synthetic Minority Over-sampling Technique},
    journal = {Journal of Artificial Intelligence Research},
    year = {2002},
    volume = {16},
    pages = {321--357}
},
@article{merkel2014docker,
  title={Docker: lightweight linux containers for consistent development and deployment},
  author={Merkel, Dirk},
  journal={Linux journal},
  volume={2014},
  number={239},
  pages={2},
  year={2014}
},
﻿@Article{Xu2018,
author={Xu, Yun
and Goodacre, Royston},
title={On Splitting Training and Validation Set: A Comparative Study of Cross-Validation, Bootstrap and Systematic Sampling for Estimating the Generalization Performance of Supervised Learning},
journal={Journal of Analysis and Testing},
year={2018},
month={Jul},
day={01},
volume={2},
number={3},
pages={249-262},
abstract={Model validation is the most important part of building a supervised model. For building a model with good generalization performance one must have a sensible data splitting strategy, and this is crucial for model validation. In this study, we conducted a comparative study on various reported data splitting methods. The MixSim model was employed to generate nine simulated datasets with different probabilities of mis-classification and variable sample sizes. Then partial least squares for discriminant analysis and support vector machines for classification were applied to these datasets. Data splitting methods tested included variants of cross-validation, bootstrapping, bootstrapped Latin partition, Kennard-Stone algorithm (K-S) and sample set partitioning based on joint X--Y distances algorithm (SPXY). These methods were employed to split the data into training and validation sets. The estimated generalization performances from the validation sets were then compared with the ones obtained from the blind test sets which were generated from the same distribution but were unseen by the training/validation procedure used in model construction. The results showed that the size of the data is the deciding factor for the qualities of the generalization performance estimated from the validation set. We found that there was a significant gap between the performance estimated from the validation set and the one from the test set for the all the data splitting methods employed on small datasets. Such disparity decreased when more samples were available for training/validation, and this is because the models were then moving towards approximations of the central limit theory for the simulated datasets used. We also found that having too many or too few samples in the training set had a negative effect on the estimated model performance, suggesting that it is necessary to have a good balance between the sizes of training set and validation set to have a reliable estimation of model performance. We also found that systematic sampling method such as K-S and SPXY generally had very poor estimation of the model performance, most likely due to the fact that they are designed to take the most representative samples first and thus left a rather poorly representative sample set for model performance estimation.},
issn={2509-4696},
doi={10.1007/s41664-018-0068-2},
url={https://doi.org/10.1007/s41664-018-0068-2}
},
@inproceedings{levy-goldberg-2014-linguistic,
    title = "Linguistic Regularities in Sparse and Explicit Word Representations",
    author = "Levy, Omer  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the Eighteenth Conference on Computational Natural Language Learning",
    month = jun,
    year = "2014",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W14-1618",
    doi = "10.3115/v1/W14-1618",
    pages = "171--180",
},
@inproceedings{10.5555/1613715.1613855,
author = {Settles, Burr and Craven, Mark},
title = {An Analysis of Active Learning Strategies for Sequence Labeling Tasks},
year = {2008},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Active learning is well-suited to many problems in natural language processing, where unlabeled data may be abundant but annotation is slow and expensive. This paper aims to shed light on the best active learning approaches for sequence labeling tasks such as information extraction and document segmentation. We survey previously used query selection strategies for sequence models, and propose several novel algorithms to address their shortcomings. We also conduct a large-scale empirical comparison using multiple corpora, which demonstrates that our proposed methods advance the state of the art.},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
pages = {1070–1079},
numpages = {10},
location = {Honolulu, Hawaii},
series = {EMNLP '08}
},
@inproceedings{10.5555/647967.741626,
author = {Scheffer, Tobias and Decomain, Christian and Wrobel, Stefan},
title = {Active Hidden Markov Models for Information Extraction},
year = {2001},
isbn = {3540425810},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Information extraction from HTML documents requires a classifier capable of assigning semantic labels to the words or word sequences to be extracted. If completely labeled documents are available for training, well-known Markov model techniques can be used to learn such classifiers. In this paper, we consider the more challenging task of learning hidden Markov models (HMMs) when only partially (sparsely) labeled documents are available for training. We first give detailed account of the task and its appropriate loss function, and show how it can be minimized given an HMM. We describe an EM style algorithm for learning HMMs from partially labeled data. We then present an active learning algorithm that selects "difficult" unlabeled tokens and asks the user to label them. We study empirically by how much active learning reduces the required data labeling effort, or increases the quality of the learned model achievable with a given amount of user effort.},
booktitle = {Proceedings of the 4th International Conference on Advances in Intelligent Data Analysis},
pages = {309–318},
numpages = {10},
series = {IDA '01}
},
@Article{Kumar2020,
author={Kumar, Punit
and Gupta, Atul},
title={Active Learning Query Strategies for Classification, Regression, and Clustering: A Survey},
journal={Journal of Computer Science and Technology},
year={2020},
month={Jul},
day={01},
volume={35},
number={4},
pages={913-945},
abstract={Generally, data is available abundantly in unlabeled form, and its annotation requires some cost. The labeling, as well as learning cost, can be minimized by learning with the minimum labeled data instances. Active learning (AL), learns from a few labeled data instances with the additional facility of querying the labels of instances from an expert annotator or oracle. The active learner uses an instance selection strategy for selecting those critical query instances, which reduce the generalization error as fast as possible. This process results in a refined training dataset, which helps in minimizing the overall cost. The key to the success of AL is query strategies that select the candidate query instances and help the learner in learning a valid hypothesis. This survey reviews AL query strategies for classification, regression, and clustering under the pool-based AL scenario. The query strategies under classification are further divided into: informative-based, representative-based, informative- and representative-based, and others. Also, more advanced query strategies based on reinforcement learning and deep learning, along with query strategies under the realistic environment setting, are presented. After a rigorous mathematical analysis of AL strategies, this work presents a comparative analysis of these strategies. Finally, implementation guide, applications, and challenges of AL are discussed.},
issn={1860-4749},
doi={10.1007/s11390-020-9487-4},
url={https://doi.org/10.1007/s11390-020-9487-4}
},
@Article{Schein2007,
author={Schein, Andrew I.
and Ungar, Lyle H.},
title={Active learning for logistic regression: an evaluation},
journal={Machine Learning},
year={2007},
month={Oct},
day={01},
volume={68},
number={3},
pages={235-265},
abstract={Which active learning methods can we expect to yield good performance in learning binary and multi-category logistic regression classifiers? Addressing this question is a natural first step in providing robust solutions for active learning across a wide variety of exponential models including maximum entropy, generalized linear, log-linear, and conditional random field models. For the logistic regression model we re-derive the variance reduction method known in experimental design circles as `A-optimality.' We then run comparisons against different variations of the most widely used heuristic schemes: query by committee and uncertainty sampling, to discover which methods work best for different classes of problems and why. We find that among the strategies tested, the experimental design methods are most likely to match or beat a random sample baseline. The heuristic alternatives produced mixed results, with an uncertainty sampling variant called margin sampling and a derivative method called QBB-MM providing the most promising performance at very low computational cost. Computational running times of the experimental design methods were a bottleneck to the evaluations. Meanwhile, evaluation of the heuristic methods lead to an accumulation of negative results. We explore alternative evaluation design parameters to test whether these negative results are merely an artifact of settings where experimental design methods can be applied. The results demonstrate a need for improved active learning methods that will provide reliable performance at a reasonable computational cost.},
issn={1573-0565},
doi={10.1007/s10994-007-5019-5},
url={https://doi.org/10.1007/s10994-007-5019-5}
}








